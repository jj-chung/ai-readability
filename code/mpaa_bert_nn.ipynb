{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-51c4FxZt0F"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install evaluate\n",
        "!pip install imblearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zmvdj5_KjP_r"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "HQa39JfUZnJw"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from transformers import TrainingArguments, Trainer\n",
        "import evaluate\n",
        "import torch\n",
        "import pandas as pd \n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.over_sampling import RandomOverSampler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "3ZYPcdcFjwSF"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Return all data available (test and train).\n",
        "\"\"\"\n",
        "def get_all_data():\n",
        "  # read excel in data frame \n",
        "  df = pd.read_excel('CLEAR_Corpus_6.01.xlsx') \n",
        "  \n",
        "  # convert a data frame to a Numpy 2D array \n",
        "  my_data = np.asarray(df) \n",
        "  return my_data\n",
        "\n",
        "\"\"\"\n",
        "Return train data.\n",
        "\"\"\"\n",
        "def train_data():\n",
        "  all_data = get_all_data()\n",
        "  np.random.seed(123)\n",
        "  np.random.shuffle(all_data)\n",
        "  split_idx = int(0.7 * all_data.shape[0])\n",
        "  print(split_idx)\n",
        "\n",
        "  return all_data[0:split_idx, :]\n",
        "\n",
        "\"\"\"\n",
        "Return test data.\n",
        "\"\"\"\n",
        "def test_data():\n",
        "  all_data = get_all_data()\n",
        "  np.random.seed(123)\n",
        "  np.random.shuffle(all_data)\n",
        "  split_idx = int(0.7 * all_data.shape[0])\n",
        "\n",
        "  return all_data[split_idx: , :]\n",
        "\n",
        "\"\"\"\n",
        "Return the array of text excerpts for training.\n",
        "\"\"\"\n",
        "def text_train_data():\n",
        "  array = train_data()\n",
        "  return array[:, 14]\n",
        "\n",
        "\"\"\"\n",
        "Return the array of text excerpts for testing.\n",
        "\"\"\"\n",
        "def text_test_data():\n",
        "  array = test_data()\n",
        "  return array[:, 14]\n",
        "\n",
        "\"\"\"\n",
        "Return MPAA ratings for training (numbers).\n",
        "\"\"\"\n",
        "def mpaa_train_data():\n",
        "  array = train_data()\n",
        "  return array[:, 12]\n",
        "\n",
        "\"\"\"\n",
        "Return MPAA ratings for testing (numbers).\n",
        "\"\"\"\n",
        "def mpaa_test_data():\n",
        "  array = test_data()\n",
        "  return array[:, 12]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "X83_12R5Qm7k"
      },
      "outputs": [],
      "source": [
        "# Random Under Sampling\n",
        "def RUS(X, y):\n",
        "    rus = RandomUnderSampler(sampling_strategy='auto')\n",
        "    X_new, Y_new = rus.fit_resample(X, y.astype('int'))\n",
        "    Y_new= Y_new.astype('int')\n",
        "    return X_new, Y_new\n",
        "\n",
        "# Random Over Sampling\n",
        "def ROS(X, y):\n",
        "    ros = RandomOverSampler(sampling_strategy='auto')\n",
        "    X_new, Y_new = ros.fit_resample(X, y.astype('int'))\n",
        "    Y_new= Y_new.astype('int')\n",
        "    return X_new, Y_new\n",
        "\n",
        "def resample(X, y, sample_type=\"Imbalanced\"):\n",
        "    if sample_type == 'Imbalanced':\n",
        "        return X,y\n",
        "    if sample_type == 'RUS':\n",
        "        return RUS(X, y)\n",
        "    if sample_type == 'ROS':\n",
        "        return ROS(X,y)\n",
        "    else:\n",
        "        print(f'{sample_type} is not recognized')\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "eMFUQ2Zkj05t"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "from datasets import Dataset\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "def mpaa_pre_processing(data_type=\"train\"):\n",
        "  if data_type == \"train\":\n",
        "    data = mpaa_train_data()\n",
        "  elif data_type == \"test\":\n",
        "    data = mpaa_test_data()\n",
        "  \n",
        "  # merge R and PG-13\n",
        "  return np.array(list(map(lambda x: 3 if x == 4 else x, data)))\n",
        "\n",
        "class MPAADataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels - 1\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "def bert_pre_processing(data_type=\"train\", sample_type=\"Imbalanced\"):\n",
        "  X_data = None\n",
        "  if data_type == \"train\":\n",
        "    X_data = text_train_data()\n",
        "  elif data_type == \"test\":\n",
        "    X_data = text_test_data()\n",
        "\n",
        "  labels = mpaa_pre_processing(data_type=data_type).astype('int')\n",
        "\n",
        "  if sample_type !=\"Imbalanced\":\n",
        "    X_data, labels = resample(X_data.reshape(-1, 1), labels, sample_type=sample_type)\n",
        "\n",
        "  # applies a different tokenizer depending on resampling bc shape is slightly different\n",
        "  if sample_type ==\"Imbalanced\":\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "    X_encodings = tokenizer(X_data.tolist(), truncation=True, padding=True)\n",
        "    return MPAADataset(X_encodings, labels)\n",
        "  else:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "    X_encodings = tokenizer(X_data.tolist(), truncation=True, padding=True, is_split_into_words=True)\n",
        "    return MPAADataset(X_encodings, labels)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sgTc7E5W1lSw"
      },
      "outputs": [],
      "source": [
        "dataset_train = bert_pre_processing(data_type=\"train\", sample_type=\"ROS\")\n",
        "dataset_test = bert_pre_processing(data_type=\"test\")\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=3)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "        output_dir=\"test_trainer\", \n",
        "        evaluation_strategy=\"epoch\",\n",
        "        num_train_epochs = 3,\n",
        "        gradient_accumulation_steps = 1,\n",
        "        per_device_train_batch_size = 8,\n",
        "        learning_rate = 5e-5,\n",
        "        logging_steps = 400\n",
        "    )\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    # metric1 = evaluate.load(\"precision\")\n",
        "    # metric2 = evaluate.load(\"recall\")\n",
        "    metric3 = evaluate.load(\"f1\")\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    # precision = metric1.compute(predictions=predictions, references=labels, average=\"weighted\")\n",
        "    # recall = metric2.compute(predictions=predictions, references=labels, average=\"weighted\")\n",
        "    # f1 = metric3.compute(predictions=predictions, references=labels, average=\"weighted\")\n",
        "    # return {\"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
        "    return metric3.compute(predictions=predictions, references=labels, average=\"weighted\")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset_train,\n",
        "    eval_dataset=dataset_test,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.1 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.1"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
